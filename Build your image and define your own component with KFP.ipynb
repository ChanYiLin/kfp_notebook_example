{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build your image and define your own component with KFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.gcp as gcp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "import kfp.components as comp\n",
    "import datetime\n",
    "\n",
    "import kubernetes as k8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Reusable Components\n",
    "### Writing the program code\n",
    "\n",
    "two imput arguments\n",
    "--model_file: \n",
    "--bucket: \n",
    "\n",
    "作為路徑名稱\n",
    "`model.save(model_file)` 、\n",
    "`gcs_path = bucket + \"/\" + model_file`\n",
    "\n",
    "gcs 儲存訓練相關日誌，供後續的 tensorboard 取用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create folders if they don't exist.\n",
    "mkdir -p tmp/reuse_components/mnist_training\n",
    "\n",
    "# Create the Python file that lists GCS blobs.\n",
    "cat > ./tmp/reuse_components/mnist_training/app.py <<HERE\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--model_file', type=str, required=True, help='Name of the model file.')\n",
    "parser.add_argument(\n",
    "    '--bucket', type=str, required=True, help='GCS bucket name.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "bucket=args.bucket\n",
    "model_file=args.model_file\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())    \n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "callbacks = [\n",
    "  tf.keras.callbacks.TensorBoard(log_dir=bucket + '/logs/' + datetime.now().date().__str__()),\n",
    "  # Interrupt training if val_loss stops improving for over 2 epochs\n",
    "  tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "]\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=5, callbacks=callbacks,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "model.save(model_file)\n",
    "\n",
    "from tensorflow import gfile\n",
    "\n",
    "gcs_path = bucket + \"/\" + model_file\n",
    "\n",
    "if gfile.Exists(gcs_path):\n",
    "    gfile.Remove(gcs_path)\n",
    "\n",
    "gfile.Copy(model_file, gcs_path)\n",
    "with open('/output.txt', 'w') as f:\n",
    "  f.write(gcs_path)\n",
    "HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Docker image with KFP and notebook in the K8s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create Dockerfile.\n",
    "cat > ./tmp/reuse_components/mnist_training/Dockerfile <<EOF\n",
    "FROM tensorflow/tensorflow:1.15.0-py3\n",
    "WORKDIR /app\n",
    "COPY . /app\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Kaniko to build image in the K8s\n",
    "We are going to use the kfp.containers.build_image_from_working_dir to build the image and push to the Container Registry (GCR), which uses **kaniko**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "IMAGE_NAME=\"mnist_training_kf_pipeline\"\n",
    "TAG=\"latest\" # \"v_$(date +%Y%m%d_%H%M%S)\"\n",
    "PROJECT_ID=\"infuseai-dev\"\n",
    "GCS_BUCKET=\"gs://jack-kubeflow-gcs\"\n",
    "\n",
    "GCR_IMAGE=\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{TAG}\".format(\n",
    "    PROJECT_ID=PROJECT_ID,\n",
    "    IMAGE_NAME=IMAGE_NAME,\n",
    "    TAG=TAG\n",
    ")\n",
    "\n",
    "builder = kfp.containers._container_builder.ContainerBuilder(\n",
    "    gcs_staging=GCS_BUCKET + \"/kfp_container_build_staging\")\n",
    "\n",
    "image_name = kfp.containers.build_image_from_working_dir(\n",
    "    image_name=GCR_IMAGE,\n",
    "    working_dir='./tmp/reuse_components/mnist_training/',\n",
    "    builder=builder\n",
    ")\n",
    "\n",
    "image_name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use docker to build image in local env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell=\n",
    "%%bash -s \"{PROJECT_ID}\"\n",
    "\n",
    "IMAGE_NAME=\"mnist_training_kf_pipeline\"\n",
    "TAG=\"latest\" # \"v_$(date +%Y%m%d_%H%M%S)\"\n",
    "\n",
    "# Create script to build docker image and push it.\n",
    "cat > ./tmp/components/mnist_training/build_image.sh <<HERE\n",
    "PROJECT_ID=\"${1}\"\n",
    "IMAGE_NAME=\"${IMAGE_NAME}\"\n",
    "TAG=\"${TAG}\"\n",
    "GCR_IMAGE=\"gcr.io/\\${PROJECT_ID}/\\${IMAGE_NAME}:\\${TAG}\"\n",
    "docker build -t \\${IMAGE_NAME} .\n",
    "docker tag \\${IMAGE_NAME} \\${GCR_IMAGE}\n",
    "docker push \\${GCR_IMAGE}\n",
    "docker image rm \\${IMAGE_NAME}\n",
    "docker image rm \\${GCR_IMAGE}\n",
    "HERE\n",
    "\n",
    "cd tmp/components/mnist_training\n",
    "bash build_image.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing your component definition file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Yaml\n",
    "the image uri should be changed according to the above docker image push output\n",
    "\n",
    "`mnist_component.yaml`\n",
    "\n",
    "```yaml\n",
    "name: Mnist training\n",
    "description: Train a mnist model and save to GCS\n",
    "inputs:\n",
    "  - name: model_file\n",
    "    description: 'Name of the model file.'\n",
    "    type: String\n",
    "  - name: bucket\n",
    "    description: 'GCS bucket name.'\n",
    "    type: String\n",
    "outputs:\n",
    "  - name: model_path\n",
    "    description: 'Trained model path.'\n",
    "    type: GCSPath\n",
    "implementation:\n",
    "  container:\n",
    "    image: ${GCR_IMAGE}\n",
    "    command: [\n",
    "      python, /app/app.py,\n",
    "      --model_file, {inputValue: model_file},\n",
    "      --bucket,     {inputValue: bucket},\n",
    "    ]\n",
    "    fileOutputs:\n",
    "      model_path: /output.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "mnist_train_op = kfp.components.load_component_from_file(os.path.join('./', 'mnist_pipeline_component.yaml'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load third party defined component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlengine_deploy_op = comp.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/3f4b80127f35e40760eeb1813ce1d3f641502222/components/gcp/ml_engine/deploy/component.yaml')\n",
    "\n",
    "# define a function to accept input argument and return the component\n",
    "def deploy(\n",
    "    project_id,\n",
    "    model_uri,\n",
    "    model_id,\n",
    "    runtime_version,\n",
    "    python_version):\n",
    "    \n",
    "    return mlengine_deploy_op(\n",
    "        model_uri=model_uri,\n",
    "        project_id=project_id, \n",
    "        model_id=model_id, \n",
    "        runtime_version=runtime_version, \n",
    "        python_version=python_version,\n",
    "        replace_existing_version=True, \n",
    "        set_default=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a function to test prediction (func to component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deployment_test(project_id: str, model_name: str, version: str) -> str:\n",
    "\n",
    "    model_name = model_name.split(\"/\")[-1]\n",
    "    version = version.split(\"/\")[-1]\n",
    "    \n",
    "    import googleapiclient.discovery\n",
    "    \n",
    "    def predict(project, model, data, version=None):\n",
    "      \"\"\"Run predictions on a list of instances.\n",
    "\n",
    "      Args:\n",
    "        project: (str), project where the Cloud ML Engine Model is deployed.\n",
    "        model: (str), model name.\n",
    "        data: ([[any]]), list of input instances, where each input instance is a\n",
    "          list of attributes.\n",
    "        version: str, version of the model to target.\n",
    "\n",
    "      Returns:\n",
    "        Mapping[str: any]: dictionary of prediction results defined by the model.\n",
    "      \"\"\"\n",
    "\n",
    "      service = googleapiclient.discovery.build('ml', 'v1')\n",
    "      name = 'projects/{}/models/{}'.format(project, model)\n",
    "\n",
    "      if version is not None:\n",
    "        name += '/versions/{}'.format(version)\n",
    "\n",
    "      response = service.projects().predict(\n",
    "          name=name, body={\n",
    "              'instances': data\n",
    "          }).execute()\n",
    "\n",
    "      if 'error' in response:\n",
    "        raise RuntimeError(response['error'])\n",
    "\n",
    "      return response['predictions']\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import json\n",
    "    \n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    result = predict(\n",
    "        project=project_id,\n",
    "        model=model_name,\n",
    "        data=x_test[0:2].tolist(),\n",
    "        version=version)\n",
    "    print(result)\n",
    "    \n",
    "    return json.dumps(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_test_op = comp.func_to_container_op(\n",
    "    func=deployment_test, \n",
    "    base_image=\"tensorflow/tensorflow:1.15.0-py3\",\n",
    "    packages_to_install=[\"google-api-python-client==1.7.8\"]) # define package to install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the pipeline\n",
    "\n",
    "What is apply\n",
    "https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.extensions.html#kfp.onprem.mount_pvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "   name='Mnist pipeline',\n",
    "   description='A toy pipeline that performs mnist model training.'\n",
    ")\n",
    "def mnist_reuse_component_deploy_pipeline(\n",
    "    project_id: str = PROJECT_ID,\n",
    "    model_path: str = 'mnist_model', \n",
    "    bucket: str = GCS_BUCKET\n",
    "):\n",
    "    train_task = mnist_train_op(\n",
    "        model_path=model_path, \n",
    "        bucket=bucket\n",
    "    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    deploy_task = deploy(\n",
    "        project_id=project_id,\n",
    "        model_uri=train_task.outputs['gcs_model_path'],\n",
    "        model_id=\"mnist\", \n",
    "        runtime_version=\"1.14\",\n",
    "        python_version=\"3.5\"\n",
    "    ).apply(gcp.use_gcp_secret('user-gcp-sa'))  \n",
    "    \n",
    "    deploy_test_task = deployment_test_op(\n",
    "        project_id=project_id,\n",
    "        model_name=deploy_task.outputs[\"model_name\"], \n",
    "        version=deploy_task.outputs[\"version_name\"],\n",
    "    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit a pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = mnist_reuse_component_deploy_pipeline\n",
    "experiment_name = 'minist_kubeflow'\n",
    "\n",
    "arguments = {\"model_path\":\"mnist_model\",\n",
    "             \"bucket\":GCS_BUCKET}\n",
    "\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
